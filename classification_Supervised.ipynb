{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lvankamamidi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import RegexpTokenizer as regextoken\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessing():\n",
    "\n",
    "    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def __init__(self, domain_stopwords, domain_trailwords, domain_impwords,repeatwords,\n",
    "                 allowed_word_types=[\"N\",\"V\",\"J\",\"R\"]):\n",
    "        self.domain_stopwords = domain_stopwords\n",
    "        self.domain_trailwords = domain_trailwords\n",
    "        self.domain_impwords = domain_impwords\n",
    "        self.repeatwords=repeatwords\n",
    "\n",
    "        self.allowed_word_types = allowed_word_types\n",
    "        self.domain_stopwords.discard(np.nan)\n",
    "        self.domain_trailwords.discard(np.nan)\n",
    "        self.domain_impwords.discard(np.nan)\n",
    "\n",
    "    def process_document(self, document, add_weightage=False):\n",
    "        document = str(document)\n",
    "        document = document.lower()\n",
    "        document = self.remove_joined_trails(document)\n",
    "        document = re.sub('[^a-zA-Z0-9\\n]',' ', document) # Remove Special Characters\n",
    "        document = re.sub('(\\\\b[A-Za-z] \\\\b|\\\\b [A-Za-z]\\\\b)', '', document) # Remove single characters\n",
    "        document = re.sub(' +', ' ', document) # Remove Extra Spaces\n",
    "        \n",
    "        document = self.lemmatize_document(document)\n",
    "        document = [word for word in document if self.check_word(word)]\n",
    "        \n",
    "        if add_weightage:\n",
    "            document = self.add_domain_impwords(document)\n",
    "        \n",
    "        document =  ' '.join([word for word in document])\n",
    "        #document = re.sub('[nan]', '', document)\n",
    "        \n",
    "        return document\n",
    "\n",
    "    def check_word(self, word):\n",
    "        if len(word)>1 and word not in self.domain_stopwords and not any(c.isdigit() for c in word):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def lemmatize_document(self, document):\n",
    "        all_words=[]\n",
    "        tokenizer = regextoken(r'\\w+')\n",
    "        words = tokenizer.tokenize(document)\n",
    "        pos = nltk.pos_tag(words)\n",
    "        for w in pos:\n",
    "            word =  w[0].lower()\n",
    "            pos_tag = w[1]\n",
    "            if pos_tag[0] in self.allowed_word_types:\n",
    "                wntag = self.get_wordnet_pos(pos_tag)\n",
    "                lemmatised_word = self.wordnet_lemmatizer.lemmatize(word,pos=wntag)\n",
    "                word = lemmatised_word if lemmatised_word in self.english_vocab else word\n",
    "                all_words.append(word)\n",
    "        return all_words\n",
    "\n",
    "    def remove_joined_trails(self, document):\n",
    "        for trail in self.domain_trailwords:\n",
    "            if trail in document:\n",
    "                document = document.replace(trail, \"\").strip()\n",
    "                document = document + ' ' + trail\n",
    "        return document\n",
    "\n",
    "    def get_wordnet_pos(self, treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def add_domain_impwords(self, document):\n",
    "        all_words = list(document)\n",
    "        for word in document:\n",
    "            if word in self.repeatwords:\n",
    "                all_words.append(word)\n",
    "                all_words.append(word)\n",
    "        return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### loading stopwords,impwords file\n",
    "stopwords_df = pd.read_excel(\"Final_CSC_Clustering.xlsx\", sheet_name=\"stopwords\")\n",
    "conolidated_stopwords = set(stopwords_df.Domain_Stopwords)\n",
    "\n",
    "conolidated_stopwords.update(set(text.ENGLISH_STOP_WORDS))\n",
    "conolidated_stopwords.update(set(stopwords.words('english')))\n",
    "\n",
    "trailwords_df = pd.read_excel(\"Final_CSC_Clustering.xlsx\", sheet_name=\"trailwords\")\n",
    "trailwords = set(trailwords_df.Trail_Words)\n",
    "\n",
    "impwords_df = pd.read_excel(\"Final_CSC_Clustering.xlsx\", sheet_name=\"impwords\")\n",
    "impwords = set(impwords_df.Domain_Impwords)\n",
    "\n",
    "repeat_df = pd.read_excel(\"Final_CSC_Clustering.xlsx\", sheet_name=\"repeat\")\n",
    "repeatwords = set(repeat_df.weightage)\n",
    "\n",
    "conolidated_stopwords = conolidated_stopwords - impwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### load training data - December clustering output file\n",
    "traindata=pd.read_excel(\"Platform ticket categorization.xlsx\")\n",
    "traindata_descript=traindata['Descript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#traindata=traindata[traindata['Category'].isna()!=True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81905, 117)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_obj=TextProcessing(conolidated_stopwords, trailwords, impwords ,repeatwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_preprocessed = traindata['Descript'].apply(preprocess_obj.process_document, args=(False,))\n",
    "traindata_preprocessed=traindata_preprocessed.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#traindata_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tf-idf matrix for train data\n",
    "tfidfconverter = TfidfVectorizer(max_features=1500, min_df=3, max_df=0.7, stop_words=stopwords.words('english'))  \n",
    "X = tfidfconverter.fit_transform(traindata_preprocessed).toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.7, max_features=1500, min_df=3,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfconverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65524, 1500) (16381, 1500) (65524,) (16381,)\n"
     ]
    }
   ],
   "source": [
    "#prepare train - test data\n",
    "y=traindata['Category']  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9867285269519566\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=100, random_state=0)  \n",
    "classifier.fit(X, y)\n",
    "print(classifier.score(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9876075941639705\n"
     ]
    }
   ],
   "source": [
    "# prediction on test data\n",
    "y_pred=classifier.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6872, 114)\n"
     ]
    }
   ],
   "source": [
    "jan_data1=pd.read_excel('NCR T3 September 2019.xlsx')\n",
    "print(jan_data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       WND1 - SBWND1SQ8P - Disk E: full - QDB02: 8550572\n",
       "1       BCN4 - SBBCN15003 - Disk D: Busy - QDB10: 1008...\n",
       "2       (2685) BCK0006E OBS2crkoralpa001b6CRK_LPA_ORAC...\n",
       "3       RESOLVED Problem 261 in environment Global Ten...\n",
       "4       RESOLVED Problem 624 in environment Global Ten...\n",
       "5       RESOLVED Problem 288 in environment Global Ten...\n",
       "6       Create a Service Now request for fulfillment q...\n",
       "7       Create a Service Now request for fulfillment q...\n",
       "8       Server         :NYHCBORA02Account      :NATAle...\n",
       "9       Server         :NYHCBORA01Account      :NATAle...\n",
       "10      Instance Name:NIMSOFT_ROBOT_nyhcbctx117vEvent ...\n",
       "11      Description:P3 URGENT Fault: TOA - server rest...\n",
       "12      Instance Name:NIMSOFT_ROBOT_NGUKSVRRTW561Event...\n",
       "13      BOR1 - SABOR10038 - Paging File Usage High - Q...\n",
       "14      Server: ukrctsrtw11024 Reporting: Robot ukrcts...\n",
       "15                             Server: jsy-shpsqlvp001   \n",
       "16      ptyagi21colapcpcn-csd01NT_Logical_Disk: C: is ...\n",
       "17      Message Suppression Off Memory Used: 97 % on E...\n",
       "18      (2686) BCK0006E OBS2CRKFSRTV001b196CRK_RTV_WIN...\n",
       "19                               Users keyboard is faulty\n",
       "20      LDAP User integration for Louise.McCarthy@belr...\n",
       "21      Job: GI MASTER Controller_Incremental failed o...\n",
       "22      GI Master Controller_Incremental in NTPD1696 s...\n",
       "23      Server: NTPA034 Reporting: Average (3 samples)...\n",
       "24      RUG1 - SVRUG10003 - QNAP usage high - QDB02: 8...\n",
       "25      9/1/2019 5:07:24 AM OS Disk Full (Percent) on ...\n",
       "26      9/1/2019 5:07:41 AM OS Disk Full (Percent) on ...\n",
       "27      RMA1 - SACCH15598 - Paging File Usage High - Q...\n",
       "28      9/1/2019 5:05:05 AM OS Disk Full (Percent) on ...\n",
       "29      9/1/2019 4:21:43 AM OS Disk Full (Percent) on ...\n",
       "                              ...                        \n",
       "6842    Assignment Notification for Ref: 7032145 Assys...\n",
       "6843                   IGRLNXNDC012 - END OF LIFE BACKUPS\n",
       "6844                 GENPACT CITRIX Latency of 20 Seconds\n",
       "6845                     Cannot launch Persistent desktop\n",
       "6846    UK: Lost Files #ukechqfile1vm I had some files...\n",
       "6847    AMP-APjkaur27IP-acihbdskd01v 31UndefinedIPDown...\n",
       "6848    Master--CSCAMPPDCBKP01 Client--ampxcdt101vm01 ...\n",
       "6849    mrb-py04p StorageArrayNetworkmrb-py04p/UDP_SAN...\n",
       "6850    user reports Corporate Network Connectivity issue\n",
       "6851    Master--CSCAMPPDCBKP01 Client--ampxcdt201vm01 ...\n",
       "6852    VST4 - SAVST18149 - Disk D: full - QDB02: 8765638\n",
       "6853                     P:sg_siteplan_directeur is full \n",
       "6854                              Not Provided in Extract\n",
       "6855    Description:**Please route this Incident to Wi...\n",
       "6856    Description:John sends IMs to David Kern and h...\n",
       "6857    Win 10 Migration - One Drive was all synced be...\n",
       "6858    Activate Tech Smith Snagit 13 for Educational ...\n",
       "6859                                   Password reset BIL\n",
       "6860    Server: NTPD2243 Reporting: Average (3 samples...\n",
       "6861    Server: NTPA1133 Reporting: Apache Tomcat 6 - ...\n",
       "6862    Description:Capture the following information:...\n",
       "6863    Instance Name:FS-NGUSXNANDC009/CEvent Text: Av...\n",
       "6864    Description:SEE  KB0010883- US: NT: How to han...\n",
       "6865    Server: nc1aq01Reporting: SQL Server Reporting...\n",
       "6866                       Copy of Windows in not genuine\n",
       "6867            ARCADP2-C-China NimsoftRobot Unresponsive\n",
       "6868              arcetlpnj002 FileSystem HighUtilization\n",
       "6869                       ETS - Outlook will not connect\n",
       "6870                             Modify Email or Calendar\n",
       "6871                                 Transaction TimedOut\n",
       "Name: Descript, Length: 6872, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jan_data1['Descript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_data=jan_data1[jan_data1['Team Name']=='Platform']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4134\n"
     ]
    }
   ],
   "source": [
    "# prediction on jan data\n",
    "jan_Documents=jan_data['Descript']\n",
    "Documentsjan=jan_Documents.apply(preprocess_obj.process_document, args=(True,))\n",
    "X_final = tfidfconverter.transform(Documentsjan)\n",
    "y_pred2 = classifier.predict(X_final)\n",
    "print(len(y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lvankamamidi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\lvankamamidi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "jan_data['predictions']=y_pred2\n",
    "jan_data['process text']=Documentsjan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan_data.to_excel('T3 September platform with categories.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clustering on nocategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nocategory=jan_data.loc[jan_data['predictions']=='No Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nocategory.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nocategory.to_csv('jan_NoCategory.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
